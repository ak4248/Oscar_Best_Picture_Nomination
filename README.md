# Oscar_Best_Picture_Nomination-
Predicting the best picture nominations at the Oscars

Summary Report - “And, the Nominations are…”

The purpose of our project is to predict the Oscar Academy Award for Best Picture nominees in the future. Each year, there are many movies with good reputation and high box offices, but eventually only no more than 10 movies are nominated. We want to know how those movies stand out from the others and which movies are potential to be nominated for the Oscar Awards. If you want to make an Oscar-nominated movie in the future, you wouldn’t want to miss our project!
 
Data is a fundamental and crucial part in our project. Since a lot of factors can influence Oscar Awards nominations, our group scraped data from various websites, including Rottentomatoes and IMDB for movies’ basic info and review scores, Numbers.com for movie budgets and box offices, wikipedia for movies’ participation times on various film festivals (Toronto Film Festival, Venice Film Festival, Sundance Film Festival), actor & directors’ previous nomination times, and movies’ nomination on other movie awards (Bafta, Golden Globes, Critics Choice, AFI awards). In the process of scraping data, we applied BeautifulSoup, requests, and selenium’s webdriver function.  Last, as we have much data collected from different sources, we used Python Pickle to serialize python object structure. 

Next, we did data cleaning and preparation. Specifically, we added variables on several  profit margins and treated variable “genres” as categorical variables. We penalized actor/director count by subtracting oscar actor(done during scrapping) and director by 1 as it could be same year. After all necessary clean and merge, we formed a final dataframe that contains 23243 rows (number of movies) with 48 columns (including  46 movie features). 

The analysis part includes visualization and text mining, displaying the factors that differentiate oscar  nominees. We used seaborn, pyplot, and scipy packages for drawing bar charts, boxplot, and line charts. In addition, we performed text mining by using python nltk, corpora, and LdaModel. We used LDA as the technique for topic modeling, with keywords resources from kaggle.com. We built two LDA models to deduce the topic distributions for both oscar nominated movies and all movies. We also performed the sentiment analysis by digging into the ratio of positive keywords versus negative keywords that appear in those movies.

The last part we did is modeling. First, we looked into the correlation among all the factors. For the factors that have strong correlations, we only take one of them into consideration. For example, IMDB rating is highly correlated with rotten tomato rating, so we dropped rotten tomato audience score before modelling.  Considering our imbalanced dataset (as there are much more movies not nominated than nominated), we tried to improve it by only selecting movies with IMDB rating higher than 6 to exclude low quality movies. We built the logistic regression model and the confusion matrix is shown in Figure 1. Instead of  randomly picking train data and test data by proportion, we think it is better to split two sets by years, which is a better simulation for future one-year prediction. We used movies from 2000 to 2014 to be the trained data, and 2015-2017 as the test data.  We used the threshold of 0.25 and got the accuracy rate of 93%. Here a default threshold of 0.5 is not used because our dataset is still imbalanced ( only a few movies get nominated every year) , even we already exclude low rating films. 


The accuracy is high (because we have a lot more non nominees,so even we predict all films to be not nominated, the accuracy can be high enough), but the precision is only 50% for logistic regression. In order to improve prediction, we performed the random forest since the model is good at classification in addition. We imported RandomForestClassifier, accuracy_score, and roc_curve. Here, because of the potentially biased accuracy, we used f-score to measure the model since f-score is a harmonic average of precision and recall. We printed f-score with different threshold and found out that when we set the f-score as 0.3, the model returns the highest f-score of 87.5%. We drew the ROC curve that has an area under the curve of 0.696. The confusion matrix we got is as shown in figure 2.

Since the correlations between other movie awards and oscar nomination are high, we considered them as strong indicator variables and remove them to run the random forest model again. At this time, we kept the threshold to be 0.3 that returns the highest f-score of 75%. In addition, we used export_graphviz, os, image and call to visualize the decision trees. At the end, we used matplotlib to visualize the important features that are more closely related with dependent variable and contribute more for variation of the dependent variable. Comparing the models, we concluded that the random forest model has higher accuracy rate and f-score. 

In conclusion, started with web scraping and building up our own dataset, this  project delivers a comprehensive analysis on factors that affect oscar nominations, as well as a meaningful model to predict future nomination results based on movie features. Hopefully it can shed light on the the recipe of oscar nominated movies. 
